# ğŸº PIVO

**Python Intelligent Version Orchestrator** - An AI-powered Git backup & metadata assistant built on a Dockerized Hadoop ecosystem.

PIVO lets you ask natural language questions about your Git repositories, compare file versions, and restore to any commit - all powered by Google Gemini.

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Docker](https://img.shields.io/badge/Docker-Required-blue)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸ¯ Features

| Capability | Example | Technology |
|------------|---------|------------|
| **Metadata Query** | "Who changed auth.py last week?" | Gemini + Hive SQL |
| **File Comparison** | "Explain changes between commits" | HDFS + difflib + Gemini |
| **Repository Restore** | "Roll back to commit abc123" | Spark + Git |
| **GitHub Backup** | Import repos into the system | Git + WebHDFS |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER                                  â”‚
â”‚                          â†“                                    â”‚
â”‚                    [ PIVO Agent ]                            â”‚
â”‚                     (main.py)                                 â”‚
â”‚                          â†“                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚              â”‚    Gemini API       â”‚                         â”‚
â”‚              â”‚  (LLM Reasoning)    â”‚                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                          â†“                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚  query_hive  â”‚  file_diff   â”‚   restore    â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â†“              â†“              â†“                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚    HIVE     â”‚  â”‚    HDFS     â”‚  â”‚    SPARK    â”‚          â”‚
â”‚  â”‚  :10000     â”‚  â”‚   :9000     â”‚  â”‚   :7077     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚               [ Docker Compose Infrastructure ]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.10+
- Git
- [Google Gemini API Key](https://makersuite.google.com/app/apikey)

### 1. Clone & Setup

```bash
git clone https://github.com/YOUR_USERNAME/PIVO.git
cd PIVO

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure

```bash
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY
```

### 3. Start Infrastructure

```bash
docker-compose up -d

# Wait for containers to be healthy (~1 min)
docker-compose ps
```

### 4. Start Background Services

Open two new terminal windows and run:

**Terminal 2: Metadata Service** (Listens for Ingestion Events)
```bash
python run_metadata_service.py
```

**Terminal 3: Audit Logger** (Captures Logs to `logs/`)
```bash
python run_audit_logger.py
```

### 5. Run PIVO Agent

**Terminal 1:**
```bash
python main.py
```

---

## ğŸ“¥ Ingesting GitHub Repositories

Before querying, you need to backup repositories into the system:

```bash
# Backup latest commit
python ingest.py --repo https://github.com/octocat/Hello-World

# Backup specific commit
python ingest.py --repo https://github.com/user/repo --commit abc1234

# Backup last 5 commits
python ingest.py --repo https://github.com/user/repo --count 5

# Private repository
python ingest.py --repo https://github.com/user/private --github-token YOUR_TOKEN
```

---

## ğŸ’¬ Example Queries

Once repositories are ingested, ask PIVO:

```
You: List all commits in the Hello-World repo
PIVO: I found 3 commits in Hello-World...

You: Who made the most changes last month?
PIVO: Based on the metadata, user "octocat" made 15 commits...

You: Compare README.md between the first and latest commit
PIVO: The changes include: Added installation instructions...
```

---

## ğŸ³ Docker Services

| Service | Port | Purpose |
|---------|------|---------|
| HDFS NameNode | 9000, 9870 | File storage |
| Hive Server | 10000 | SQL queries |
| Kafka | 9092 | Message queue |
| Spark Master | 7077, 8080 | Processing |

**Web UIs:**
- HDFS: http://localhost:9870
- Spark: http://localhost:8080

---

## ğŸ“ Project Structure

```
PIVO/
â”œâ”€â”€ docker-compose.yml      # Hadoop ecosystem
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â”œâ”€â”€ main.py                # PIVO agent CLI
â”œâ”€â”€ ingest.py              # GitHub import CLI
â”œâ”€â”€ pivo/
â”‚   â”œâ”€â”€ agent.py           # Gemini orchestrator
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ query_hive.py  # Text-to-SQL
â”‚   â”‚   â”œâ”€â”€ file_diff.py   # Smart diff
â”‚   â”‚   â””â”€â”€ restore.py     # Spark restore
â”‚   â””â”€â”€ ingest/
â”‚       â”œâ”€â”€ github_cloner.py
â”‚       â”œâ”€â”€ hdfs_uploader.py
â”‚       â””â”€â”€ hive_cataloger.py
â””â”€â”€ spark_jobs/
    â””â”€â”€ restore_job.py     # Spark job script
```

---

## ğŸ”§ Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `GEMINI_API_KEY` | Google Gemini API key | Required |
| `PIVO_MODEL` | Gemini model | gemini-2.0-flash |
| `HDFS_HOST` | HDFS hostname | localhost |
| `HIVE_HOST` | Hive hostname | localhost |

---

## ğŸ› ï¸ Development

```bash
# Run tests
python -m pytest tests/

# Check code style
python -m flake8 pivo/

# Format code
python -m black pivo/
```

---

## ğŸ“œ License

MIT License - feel free to use this for your Big Data course projects!

---

## ğŸ™ Acknowledgments

Built with:
- [Google Gemini](https://ai.google.dev/) - LLM
- [Apache Hadoop](https://hadoop.apache.org/) - HDFS
- [Apache Hive](https://hive.apache.org/) - SQL
- [Apache Spark](https://spark.apache.org/) - Processing
- [Apache Kafka](https://kafka.apache.org/) - Messaging
